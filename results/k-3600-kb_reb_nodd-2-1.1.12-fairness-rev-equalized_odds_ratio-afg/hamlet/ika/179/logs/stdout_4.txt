Sensitive groups ready!
Theory ready!
Theory:
step(normalization) :- true.
step(features) :- true.
step(mitigation) :- true.
step(rebalancing) :- true.
step(classification) :- true.
operator(normalization, power_transformer) :- true.
operator(normalization, robust_scaler) :- true.
operator(normalization, standard) :- true.
operator(normalization, minmax) :- true.
operator(features, select_k_best) :- true.
operator(features, pca) :- true.
operator(mitigation, corr_remover) :- true.
operator(mitigation, lfr) :- true.
operator(rebalancing, near_miss) :- true.
operator(rebalancing, smote) :- true.
operator(classification, knn) :- true.
operator(classification, nn) :- true.
operator(classification, rf) :- true.
hyperparameter(robust_scaler, with_centering, choice) :- true.
hyperparameter(robust_scaler, with_scaling, choice) :- true.
hyperparameter(standard, with_mean, choice) :- true.
hyperparameter(standard, with_std, choice) :- true.
hyperparameter(select_k_best, k, randint) :- true.
hyperparameter(pca, n_components, randint) :- true.
hyperparameter(corr_remover, alpha, choice) :- true.
hyperparameter(lfr, n_prototypes, choice) :- true.
hyperparameter(lfr, reconstruct_weight, choice) :- true.
hyperparameter(lfr, fairness_weight, choice) :- true.
hyperparameter(near_miss, n_neighbors, randint) :- true.
hyperparameter(smote, k_neighbors, randint) :- true.
hyperparameter(knn, n_neighbors, randint) :- true.
hyperparameter(knn, weights, choice) :- true.
hyperparameter(knn, metric, choice) :- true.
hyperparameter(nn, n_hidden_layers, choice) :- true.
hyperparameter(nn, n_neurons, choice) :- true.
hyperparameter(nn, activation, choice) :- true.
hyperparameter(nn, solver, choice) :- true.
hyperparameter(nn, alpha, choice) :- true.
hyperparameter(nn, learning_rate, choice) :- true.
hyperparameter(rf, n_estimators, choice) :- true.
hyperparameter(rf, max_depth, randint) :- true.
hyperparameter(rf, max_features, randint) :- true.
hyperparameter(rf, min_samples_split, randint) :- true.
hyperparameter(rf, max_leaf_nodes, randint) :- true.
hyperparameter(rf, bootstrap, choice) :- true.
hyperparameter(rf, criterion, choice) :- true.
domain(robust_scaler, with_centering, [true, false]) :- true.
domain(robust_scaler, with_scaling, [true, false]) :- true.
domain(standard, with_mean, [true, false]) :- true.
domain(standard, with_std, [true, false]) :- true.
domain(select_k_best, k, [1, 10]) :- true.
domain(pca, n_components, [1, 8]) :- true.
domain(corr_remover, alpha, [0.25, 0.5, 0.75, 1.0]) :- true.
domain(lfr, n_prototypes, [10, 50, 100, 200]) :- true.
domain(lfr, reconstruct_weight, [0.1, 0.5, 1.0, 2.0]) :- true.
domain(lfr, fairness_weight, [0.1, 0.5, 1.0, 2.0]) :- true.
domain(near_miss, n_neighbors, [1, 4]) :- true.
domain(smote, k_neighbors, [5, 8]) :- true.
domain(knn, n_neighbors, [3, 20]) :- true.
domain(knn, weights, [uniform, distance]) :- true.
domain(knn, metric, [minkowski, euclidean, manhattan]) :- true.
domain(nn, n_hidden_layers, [1, 5, 10, 20]) :- true.
domain(nn, n_neurons, [5, 10, 25, 50]) :- true.
domain(nn, activation, [logistic, tanh, relu]) :- true.
domain(nn, solver, [lbfgs, sgd, adam]) :- true.
domain(nn, alpha, [0.0001, 0.001, 0.01, 0.00001]) :- true.
domain(nn, learning_rate, [constant, invscaling, adaptive]) :- true.
domain(rf, n_estimators, [10, 25, 50, 75, 100]) :- true.
domain(rf, max_depth, [1, 5]) :- true.
domain(rf, max_features, [1, 4]) :- true.
domain(rf, min_samples_split, [2, 6]) :- true.
domain(rf, max_leaf_nodes, [2, 6]) :- true.
domain(rf, bootstrap, [true, false]) :- true.
domain(rf, criterion, [gini, entropy]) :- true.
dataset(adult) :- true.
metric(balanced_accuracy) :- true.
fairness_metric(equalized_odds_ratio) :- true.
sensitive_feature(sex, ['Male', 'Female']) :- true.
sensitive_feature(race, ['Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White']) :- true.
':=>'(mc0, unbalanced_dataset) :- true.
pipeline([Step_1_0], ZZ_0) :- (step(Step_1_0), '\\='(Step_1_0, classification), operator(classification, ZZ_0)).
pipeline([Step_1_1, Step_2_0], ZZ_1) :- (step(Step_1_1), step(Step_2_0), '\\='(Step_1_1, classification), '\\='(Step_2_0, classification), '\\='(Step_1_1, Step_2_0), operator(classification, ZZ_1)).
pipeline([Step_1_2, Step_2_1, Step_3_0], ZZ_2) :- (step(Step_1_2), step(Step_2_1), step(Step_3_0), '\\='(Step_1_2, classification), '\\='(Step_2_1, classification), '\\='(Step_3_0, classification), '\\='(Step_1_2, Step_2_1), '\\='(Step_1_2, Step_3_0), '\\='(Step_2_1, Step_3_0), operator(classification, ZZ_2)).
pipeline([Step_1_3, Step_2_2, Step_3_1, Step_4_0], ZZ_3) :- (step(Step_1_3), step(Step_2_2), step(Step_3_1), step(Step_4_0), '\\='(Step_1_3, classification), '\\='(Step_2_2, classification), '\\='(Step_3_1, classification), '\\='(Step_4_0, classification), '\\='(Step_1_3, Step_2_2), '\\='(Step_1_3, Step_3_1), '\\='(Step_1_3, Step_4_0), '\\='(Step_2_2, Step_3_1), '\\='(Step_2_2, Step_4_0), '\\='(Step_3_1, Step_4_0), operator(classification, ZZ_3)).
':=>'(p204344, sensitive_group(['Male'])) :- true.
':=>'(p835629, sensitive_group(['Female'])) :- true.
':=>'(p688071, sensitive_group(['Amer-Indian-Eskimo'])) :- true.
':=>'(p519141, sensitive_group(['Asian-Pac-Islander'])) :- true.
':=>'(p990505, sensitive_group(['Black'])) :- true.
':=>'(p126145, sensitive_group(['Other'])) :- true.
':=>'(p509280, sensitive_group(['White'])) :- true.
':=>'(p398918, sensitive_group(['Male', 'Amer-Indian-Eskimo'])) :- true.
':=>'(p21719, sensitive_group(['Male', 'Asian-Pac-Islander'])) :- true.
':=>'(p5059, sensitive_group(['Male', 'Black'])) :- true.
':=>'(p9416, sensitive_group(['Male', 'Other'])) :- true.
':=>'(p975321, sensitive_group(['Male', 'White'])) :- true.
':=>'(p454819, sensitive_group(['Female', 'Amer-Indian-Eskimo'])) :- true.
':=>'(p432217, sensitive_group(['Female', 'Asian-Pac-Islander'])) :- true.
':=>'(p720934, sensitive_group(['Female', 'Black'])) :- true.
':=>'(p320389, sensitive_group(['Female', 'Other'])) :- true.
':=>'(p756894, sensitive_group(['Female', 'White'])) :- true.

Checking Config
Exporting Config
Saving Graph
Exporting AutoML input
Exporting Space
Exporting Constraints
Input created for iteration Config(iteration=4, dataset=adult, metric=balanced_accuracy, fairnessMetric=equalized_odds_ratio, sensitiveFeatures=[sex, race], mode=max, batchSize=999999, timeBudget=900, seed=42)
Here is the standard output/error of the command:

884a898d7c3175336a16a59b54ffdf0b334ba51aa08708903a32b371d9e8def4
Here is the standard output/error of the command:

Here is the standard output/error of the command:

WARNING:root:No module named 'rpy2': FairAdapt will be unavailable. To install, run:
pip install 'aif360[FairAdapt]'
INFO:flaml.tune.searcher.blendsearch:No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'. More info can be found at https://microsoft.github.io/FLAML/docs/FAQ#about-low_cost_partial_config-in-tune
AutoML: starting optimization.
23
success
{'classification': {'type': np.str_('KNeighborsClassifier'), 'metric': np.str_('manhattan'), 'n_neighbors': 9, 'weights': np.str_('distance')}, 'features': {'type': np.str_('SelectKBest'), 'k': 2}, 'mitigation': {'type': np.str_('LFR_wrapper'), 'fairness_weight': np.float64(0.5), 'n_prototypes': np.int64(10), 'reconstruct_weight': np.float64(0.1)}, 'rebalancing': {'type': np.str_('SMOTE'), 'k_neighbors': 6}, 'normalization': {'type': np.str_('MinMaxScaler')}, 'prototype': np.str_('rebalancing_normalization_mitigation_features_classification')}
{'equalized_odds_ratio': np.float64(0.39132536787098215), 'balanced_accuracy': np.float64(0.622536588478287), 'by_group': {'Asian-Pac-Islander_Female': np.float64(0.69), 'Asian-Pac-Islander_Male': np.float64(0.79), 'Black_Female': np.float64(0.43), 'Black_Male': np.float64(0.58), 'White_Female': np.float64(0.49), 'White_Male': np.float64(0.68)}, 'status': 'success', 'total_time': 255.9454584121704, 'fit_time': np.float64(26.22820963859558), 'score_time': np.float64(1.2819786548614502), 'absolute_time': 1746704407.1213348, 'flatten_equalized_odds_ratio': '0.37_0.0_0.69_0.35_0.54', 'flatten_balanced_accuracy': '0.67_0.5_0.62_0.66_0.66'}
The result for {'classification': {'type': np.str_('KNeighborsClassifier'), 'metric': np.str_('euclidean'), 'n_neighbors': 7, 'weights': np.str_('distance')}, 'mitigation': {'type': np.str_('LFR_wrapper'), 'fairness_weight': np.float64(0.5), 'n_prototypes': np.int64(50), 'reconstruct_weight': np.float64(2.0)}, 'rebalancing': {'type': np.str_('SMOTE'), 'k_neighbors': 6}, 'features': {'type': np.str_('FunctionTransformer')}, 'normalization': {'type': np.str_('MinMaxScaler')}, 'prototype': np.str_('rebalancing_features_mitigation_normalization_classification')} was NaN

Something went wrong
Traceback (most recent call last):
  File "/home/automl/hamlet/objective.py", line 649, in objective
    raise Exception(f"The result for {config} was NaN")
Exception: The result for {'classification': {'type': np.str_('KNeighborsClassifier'), 'metric': np.str_('euclidean'), 'n_neighbors': 7, 'weights': np.str_('distance')}, 'mitigation': {'type': np.str_('LFR_wrapper'), 'fairness_weight': np.float64(0.5), 'n_prototypes': np.int64(50), 'reconstruct_weight': np.float64(2.0)}, 'rebalancing': {'type': np.str_('SMOTE'), 'k_neighbors': 6}, 'features': {'type': np.str_('FunctionTransformer')}, 'normalization': {'type': np.str_('MinMaxScaler')}, 'prototype': np.str_('rebalancing_features_mitigation_normalization_classification')} was NaN

24
fail
{'classification': {'type': np.str_('KNeighborsClassifier'), 'metric': np.str_('euclidean'), 'n_neighbors': 7, 'weights': np.str_('distance')}, 'mitigation': {'type': np.str_('LFR_wrapper'), 'fairness_weight': np.float64(0.5), 'n_prototypes': np.int64(50), 'reconstruct_weight': np.float64(2.0)}, 'rebalancing': {'type': np.str_('SMOTE'), 'k_neighbors': 6}, 'features': {'type': np.str_('FunctionTransformer')}, 'normalization': {'type': np.str_('MinMaxScaler')}, 'prototype': np.str_('rebalancing_features_mitigation_normalization_classification')}
{'equalized_odds_ratio': -inf, 'balanced_accuracy': np.float64(0.49995500000653037), 'by_group': {'Asian-Pac-Islander_Female': np.float64(0.0), 'Asian-Pac-Islander_Male': np.float64(0.0), 'Black_Female': np.float64(0.0), 'Black_Male': np.float64(0.0), 'White_Female': np.float64(0.25), 'White_Male': np.float64(0.5)}, 'status': 'fail', 'total_time': 253.5635814666748, 'fit_time': np.float64(41.52139573097229), 'score_time': np.float64(4.53067102432251), 'absolute_time': 1746704660.7876213, 'flatten_equalized_odds_ratio': 'nan_nan_nan_nan_0.0', 'flatten_balanced_accuracy': '0.5_0.5_0.5_0.5_0.5'}

Something went wrong
Traceback (most recent call last):
  File "/home/automl/hamlet/objective.py", line 609, in objective
    scores = cross_validate(
             ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 216, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/sklearn/model_selection/_validation.py", line 431, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File "/usr/local/lib/python3.12/site-packages/sklearn/model_selection/_validation.py", line 517, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 5 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
5 fits failed with the following error:
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/sklearn/model_selection/_validation.py", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/usr/local/lib/python3.12/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/imblearn/pipeline.py", line 518, in fit
    Xt, yt = self._fit(X, y, routed_params, raw_params=params)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/imblearn/pipeline.py", line 430, in _fit
    X, fitted_transformer = fit_transform_one_cached(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/joblib/memory.py", line 312, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/imblearn/pipeline.py", line 1383, in _fit_transform_one
    res = transformer.fit_transform(X, y, **params.get("fit_transform", {}))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/sklearn/utils/_set_output.py", line 319, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/automl/hamlet/transformers/lfr_wrapper.py", line 76, in fit_transform
    df_X, df_y = self._prepare_dataset(X, y, fit=True)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/automl/hamlet/transformers/lfr_wrapper.py", line 37, in _prepare_dataset
    raise Exception("No sensitive features to mitigate")
Exception: No sensitive features to mitigate


25
fail
{'classification': {'type': np.str_('MLPClassifier'), 'activation': np.str_('logistic'), 'alpha': np.float64(0.01), 'learning_rate': np.str_('invscaling'), 'n_hidden_layers': np.int64(1), 'n_neurons': np.int64(10), 'solver': np.str_('adam')}, 'features': {'type': np.str_('SelectKBest'), 'k': 5}, 'mitigation': {'type': np.str_('LFR_wrapper'), 'fairness_weight': np.float64(1.0), 'n_prototypes': np.int64(50), 'reconstruct_weight': np.float64(0.1)}, 'normalization': {'type': np.str_('RobustScaler'), 'with_centering': np.False_, 'with_scaling': np.True_}, 'rebalancing': {'type': np.str_('NearMiss'), 'n_neighbors': 2}, 'prototype': np.str_('rebalancing_normalization_features_mitigation_classification')}
{'equalized_odds_ratio': -inf, 'balanced_accuracy': -inf, 'by_group': {'Asian-Pac-Islander_Female': -inf, 'Asian-Pac-Islander_Male': -inf, 'Black_Female': -inf, 'Black_Male': -inf, 'White_Female': -inf, 'White_Male': -inf}, 'status': 'fail', 'total_time': 1.1798954010009766, 'fit_time': 0, 'score_time': 0, 'absolute_time': 1746704662.0772939}

Something went wrong
Traceback (most recent call last):
  File "/home/automl/hamlet/objective.py", line 585, in objective
    pipeline = self._instantiate_pipeline(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/automl/hamlet/objective.py", line 485, in _instantiate_pipeline
    _check_coherence(prototype, config)
  File "/home/automl/hamlet/objective.py", line 96, in _check_coherence
    raise Exception(f"""PCA before {config["mitigation"]["type"]}""")
Exception: PCA before LFR_wrapper

26
fail
{'classification': {'type': np.str_('MLPClassifier'), 'activation': np.str_('tanh'), 'alpha': np.float64(0.0001), 'learning_rate': np.str_('constant'), 'n_hidden_layers': np.int64(1), 'n_neurons': np.int64(5), 'solver': np.str_('lbfgs')}, 'features': {'type': np.str_('PCA'), 'n_components': 3}, 'mitigation': {'type': np.str_('LFR_wrapper'), 'fairness_weight': np.float64(0.1), 'n_prototypes': np.int64(10), 'reconstruct_weight': np.float64(0.1)}, 'normalization': {'type': np.str_('StandardScaler'), 'with_mean': np.True_, 'with_std': np.False_}, 'rebalancing': {'type': np.str_('NearMiss'), 'n_neighbors': 2}, 'prototype': np.str_('rebalancing_features_mitigation_normalization_classification')}
{'equalized_odds_ratio': -inf, 'balanced_accuracy': -inf, 'by_group': {'Asian-Pac-Islander_Female': -inf, 'Asian-Pac-Islander_Male': -inf, 'Black_Female': -inf, 'Black_Male': -inf, 'White_Female': -inf, 'White_Male': -inf}, 'status': 'fail', 'total_time': 0.0004935264587402344, 'fit_time': 0, 'score_time': 0, 'absolute_time': 1746704662.1776888}
27
success
{'classification': {'type': np.str_('RandomForestClassifier'), 'bootstrap': np.False_, 'criterion': np.str_('entropy'), 'max_depth': 4, 'max_features': 2, 'max_leaf_nodes': 5, 'min_samples_split': 6, 'n_estimators': np.int64(50)}, 'normalization': {'type': np.str_('StandardScaler'), 'with_mean': np.True_, 'with_std': np.False_}, 'rebalancing': {'type': np.str_('SMOTE'), 'k_neighbors': 6}, 'features': {'type': np.str_('FunctionTransformer')}, 'mitigation': {'type': np.str_('FunctionTransformer')}, 'prototype': np.str_('mitigation_rebalancing_features_normalization_classification')}
{'equalized_odds_ratio': np.float64(0.023996830593595992), 'balanced_accuracy': np.float64(0.7842144101635242), 'by_group': {'Asian-Pac-Islander_Female': np.float64(0.13), 'Asian-Pac-Islander_Male': np.float64(0.9), 'Black_Female': np.float64(0.02), 'Black_Male': np.float64(0.4), 'White_Female': np.float64(0.06), 'White_Male': np.float64(0.99)}, 'status': 'success', 'total_time': 6.9563374519348145, 'fit_time': np.float64(1.2400521278381347), 'score_time': np.float64(0.022452688217163085), 'absolute_time': 1746704669.238135, 'flatten_equalized_odds_ratio': '0.02_0.01_0.04_0.03_0.02', 'flatten_balanced_accuracy': '0.79_0.78_0.78_0.79_0.78'}
28
success
{'classification': {'type': np.str_('MLPClassifier'), 'activation': np.str_('logistic'), 'alpha': np.float64(0.0001), 'learning_rate': np.str_('constant'), 'n_hidden_layers': np.int64(1), 'n_neurons': np.int64(50), 'solver': np.str_('lbfgs')}, 'normalization': {'type': np.str_('RobustScaler'), 'with_centering': np.False_, 'with_scaling': np.True_}, 'rebalancing': {'type': np.str_('NearMiss'), 'n_neighbors': 2}, 'features': {'type': np.str_('FunctionTransformer')}, 'mitigation': {'type': np.str_('FunctionTransformer')}, 'prototype': np.str_('features_rebalancing_mitigation_normalization_classification')}
{'equalized_odds_ratio': np.float64(0.18571806413291764), 'balanced_accuracy': np.float64(0.793167197419644), 'by_group': {'Asian-Pac-Islander_Female': np.float64(0.53), 'Asian-Pac-Islander_Male': np.float64(1.0), 'Black_Female': np.float64(0.19), 'Black_Male': np.float64(0.48), 'White_Female': np.float64(0.23), 'White_Male': np.float64(0.6)}, 'status': 'success', 'total_time': 233.76499676704407, 'fit_time': np.float64(46.56640453338623), 'score_time': np.float64(0.0393033504486084), 'absolute_time': 1746704903.1013355, 'flatten_equalized_odds_ratio': '0.2_0.13_0.2_0.21_0.2', 'flatten_balanced_accuracy': '0.8_0.79_0.79_0.8_0.79'}
The result for {'classification': {'type': np.str_('MLPClassifier'), 'activation': np.str_('logistic'), 'alpha': np.float64(0.0001), 'learning_rate': np.str_('adaptive'), 'n_hidden_layers': np.int64(10), 'n_neurons': np.int64(10), 'solver': np.str_('lbfgs')}, 'mitigation': {'type': np.str_('LFR_wrapper'), 'fairness_weight': np.float64(1.0), 'n_prototypes': np.int64(10), 'reconstruct_weight': np.float64(0.1)}, 'features': {'type': np.str_('FunctionTransformer')}, 'normalization': {'type': np.str_('FunctionTransformer')}, 'prototype': np.str_('mitigation_features_normalization_rebalancing_classification'), 'rebalancing': {'type': np.str_('FunctionTransformer')}} was NaN

Something went wrong
Traceback (most recent call last):
  File "/home/automl/hamlet/objective.py", line 649, in objective
    raise Exception(f"The result for {config} was NaN")
Exception: The result for {'classification': {'type': np.str_('MLPClassifier'), 'activation': np.str_('logistic'), 'alpha': np.float64(0.0001), 'learning_rate': np.str_('adaptive'), 'n_hidden_layers': np.int64(10), 'n_neurons': np.int64(10), 'solver': np.str_('lbfgs')}, 'mitigation': {'type': np.str_('LFR_wrapper'), 'fairness_weight': np.float64(1.0), 'n_prototypes': np.int64(10), 'reconstruct_weight': np.float64(0.1)}, 'features': {'type': np.str_('FunctionTransformer')}, 'normalization': {'type': np.str_('FunctionTransformer')}, 'prototype': np.str_('mitigation_features_normalization_rebalancing_classification'), 'rebalancing': {'type': np.str_('FunctionTransformer')}} was NaN

29
fail
{'classification': {'type': np.str_('MLPClassifier'), 'activation': np.str_('logistic'), 'alpha': np.float64(0.0001), 'learning_rate': np.str_('adaptive'), 'n_hidden_layers': np.int64(10), 'n_neurons': np.int64(10), 'solver': np.str_('lbfgs')}, 'mitigation': {'type': np.str_('LFR_wrapper'), 'fairness_weight': np.float64(1.0), 'n_prototypes': np.int64(10), 'reconstruct_weight': np.float64(0.1)}, 'features': {'type': np.str_('FunctionTransformer')}, 'normalization': {'type': np.str_('FunctionTransformer')}, 'prototype': np.str_('mitigation_features_normalization_rebalancing_classification'), 'rebalancing': {'type': np.str_('FunctionTransformer')}}
{'equalized_odds_ratio': -inf, 'balanced_accuracy': np.float64(0.5), 'by_group': {'Asian-Pac-Islander_Female': -inf, 'Asian-Pac-Islander_Male': -inf, 'Black_Female': -inf, 'Black_Male': -inf, 'White_Female': -inf, 'White_Male': -inf}, 'status': 'fail', 'total_time': 333.72624254226685, 'fit_time': np.float64(66.17346868515014), 'score_time': np.float64(0.23643040657043457), 'absolute_time': 1746705236.9856868, 'flatten_equalized_odds_ratio': 'nan_nan_nan_nan_nan', 'flatten_balanced_accuracy': '0.5_0.5_0.5_0.5_0.5'}
AutoML: optimization done.
AutoML: miner done.
AutoML: export done.
Here is the standard output/error of the command:

Here is the standard output/error of the command:

Here is the standard output/error of the command:

hamlet_1076680916
AutoML execution ended
